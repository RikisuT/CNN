{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "num_workers = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the dataset directory\n",
    "data_dir = \"/home/rikisu/NNDL/CNN/cell_images\"\n",
    "\n",
    "# Set the number of training epochs (you can increase this later for better accuracy)\n",
    "epochs = 2\n",
    "\n",
    "# Automatically set the number of worker processes for data loading\n",
    "# based on the number of available CPU cores\n",
    "num_workers = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN model\n",
    "class MalariaCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MalariaCNN, self).__init__()\n",
    "\n",
    "        #---------- CONVOLUTIONAL LAYERS ----------#\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        #---------- FULLY CONNECTED LAYERS ----------#\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 128)  # 128 feature maps * 16x16 spatial size\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 2)  # Binary classification\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu1(self.conv1(x)))  # 128x128 -> 64x64\n",
    "        x = self.pool(self.relu2(self.conv2(x)))  # 64x64 -> 32x32\n",
    "        x = self.pool(self.relu3(self.conv3(x)))  # 32x32 -> 16x16\n",
    "        x = x.view(x.size(0), -1)                 # Flatten\n",
    "        x = self.relu4(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------- DATA PREPARATION ----------#\n",
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize all images to 128 x 128\n",
    "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize pixel values\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)  # Expects data in folders named by class\n",
    "train_size = int(0.8 * len(dataset))  # 80% training data\n",
    "test_size = len(dataset) - train_size  # 20% testing data\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=num_workers, pin_memory=True)#---------- MODEL SETUP ----------#\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "model = MalariaCNN().to(device)  # Move model to GPU if available\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training ---\n"
     ]
    }
   ],
   "source": [
    "#---------- TRAINING LOOP ----------#\n",
    "\n",
    "\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "test_accuracies = [] # Initialize list to store test accuracies per epoch\n",
    "\n",
    "\n",
    "# Enable mixed precision training\n",
    "scaler = torch.amp.GradScaler()  # Fixed: removed 'cuda' parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Loss: 0.2367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:   0%|          | 0/689 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy after epoch 1: 95.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Loss: 0.1370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy after epoch 2: 95.85%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "\n",
    "    for images, labels in progress_bar:\n",
    "        # Move data to device\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        with torch.amp.autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):  # Mixed precision\n",
    "            outputs = model(images)  # Get model predictions\n",
    "            loss = criterion(outputs, labels)  # Calculate loss\n",
    "\n",
    "        # Backward pass with gradient scaling for mixed precision\n",
    "        scaler.scale(loss).backward()  # Compute gradients\n",
    "        scaler.step(optimizer)  # Update weights\n",
    "        scaler.update()  # Update scaler\n",
    "\n",
    "        # Track loss\n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=running_loss / (progress_bar.n + 1))\n",
    "\n",
    "    # Calculate average loss for this epoch\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    #---------- EVALUATION ----------#\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            outputs = model(images)  # Get model predictions\n",
    "            _, predicted = torch.max(outputs, 1)  # Get class with highest probability\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "    # Calculate and store accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    test_accuracies.append(accuracy)\n",
    "    print(f\"Test Accuracy after epoch {epoch+1}: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score cannot be calculated for this setup.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2601  127]\n",
      " [ 102 2682]]\n",
      "Final Test Loss: 0.1324\n",
      "Final Test Accuracy: 95.85%\n",
      "Final Test Precision: 0.9585\n",
      "Final Test Recall: 0.9585\n",
      "Final Test F1-Score: 0.9585\n",
      "Final Test AUC: 0.0000\n"
     ]
    }
   ],
   "source": [
    "#---------- FINAL EVALUATION ON TEST SET ----------#\n",
    "model.eval()\n",
    "test_all_preds = []\n",
    "test_all_labels = []\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        test_all_preds.extend(predicted.cpu().numpy())\n",
    "        test_all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_accuracy = 100 * correct / total\n",
    "test_precision = precision_score(test_all_labels, test_all_preds, average='weighted', zero_division=0)\n",
    "test_recall = recall_score(test_all_labels, test_all_preds, average='weighted', zero_division=0)\n",
    "test_f1 = f1_score(test_all_labels, test_all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "# For AUC, we need probabilities for each class.\n",
    "# Assuming binary classification (0 and 1), we can take the probability of class 1.\n",
    "try:\n",
    "    probabilities = F.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "    test_auc = roc_auc_score(test_all_labels, probabilities)\n",
    "except ValueError:\n",
    "    print(\"AUC score cannot be calculated for this setup.\")\n",
    "    test_auc = 0.0\n",
    "\n",
    "cm = confusion_matrix(test_all_labels, test_all_preds)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"Final Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"Final Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Final Test Recall: {test_recall:.4f}\")\n",
    "print(f\"Final Test F1-Score: {test_f1:.4f}\")\n",
    "print(f\"Final Test AUC: {test_auc:.4f}\")\n",
    "\n",
    "\n",
    "# --- Store Results for Plotting ---\n",
    "results = {\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'val_accuracies': val_accuracies,\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'test_precision': test_precision,\n",
    "    'test_recall': test_recall,\n",
    "    'test_f1': test_f1,\n",
    "    'test_auc': test_auc,\n",
    "    'confusion_matrix': cm\n",
    "}\n",
    "\n",
    "# Get class names from the dataset\n",
    "class_names = dataset.classes\n",
    "# Define the save directory for the plot\n",
    "save_dir = \"results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3167887546.py, line 105)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 105\u001b[0;36m\u001b[0m\n\u001b[0;31m    Gemini can make mistakes, so double-check it\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def plot_results(results_dict, class_names_list, save_path):\n",
    "    # Get actual epochs run\n",
    "    num_epochs_run = len(results_dict.get('train_losses', []))\n",
    "    epochs_axis = range(1, num_epochs_run + 1)\n",
    "\n",
    "    plt.figure(figsize=(16, 12))\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs_axis, results_dict.get('train_losses', []), label='Training Loss', marker='o', linestyle='-')\n",
    "    if 'val_losses' in results_dict and results_dict['val_losses']:\n",
    "        plt.plot(epochs_axis, results_dict.get('val_losses', []), label='Validation Loss', marker='o', linestyle='-')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(epochs_axis)\n",
    "\n",
    "    # Plot validation accuracy (only if we have it)\n",
    "    plt.subplot(2, 2, 2)\n",
    "    if 'val_accuracies' in results_dict and results_dict['val_accuracies']:\n",
    "        plt.plot(epochs_axis, results_dict.get('val_accuracies', []), label='Validation Accuracy',\n",
    "                 color='g', marker='o', linestyle='-')\n",
    "    # Add test accuracy as a horizontal line if available\n",
    "    if 'test_accuracy' in results_dict:\n",
    "        plt.axhline(y=results_dict['test_accuracy'], color='r', linestyle='--',\n",
    "                    label=f'Test Accuracy: {results_dict[\"test_accuracy\"]:.2f}%')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(epochs_axis)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    cm_plot = results_dict.get('confusion_matrix')\n",
    "    if cm_plot is not None and len(class_names_list) > 0:\n",
    "        plt.subplot(2, 2, 3)\n",
    "        im = plt.imshow(cm_plot, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        plt.title('Confusion Matrix (Test Set)')\n",
    "        plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "        tick_marks = np.arange(len(class_names_list))\n",
    "        plt.xticks(tick_marks, class_names_list, rotation=45, ha=\"right\")\n",
    "        plt.yticks(tick_marks, class_names_list)\n",
    "\n",
    "        # Add text annotations\n",
    "        thresh = cm_plot.max() / 2.\n",
    "        for i in range(cm_plot.shape[0]):\n",
    "            for j in range(cm_plot.shape[1]):\n",
    "                plt.text(j, i, format(cm_plot[i, j], 'd'),\n",
    "                         ha=\"center\", va=\"center\",\n",
    "                         color=\"white\" if cm_plot[i, j] > thresh else \"black\")\n",
    "\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.tight_layout(rect=[0, 0, 0.9, 1])  # Adjust layout for confusion matrix\n",
    "\n",
    "    # Plot additional test metrics bar chart\n",
    "    plt.subplot(2, 2, 4)\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']\n",
    "    values = [\n",
    "        results_dict.get('test_accuracy', 0) / 100.0,  # Scale accuracy to 0-1 range\n",
    "        results_dict.get('test_precision', 0),\n",
    "        results_dict.get('test_recall', 0),\n",
    "        results_dict.get('test_f1', 0),\n",
    "        results_dict.get('test_auc', 0)\n",
    "    ]\n",
    "    bars = plt.bar(metrics, values, color=['#1f77b4', '#2ca02c', '#d62728', '#9467bd', '#ff7f0e'])\n",
    "    plt.title('Final Test Set Metrics (Best Model)')\n",
    "    plt.ylabel('Score')\n",
    "\n",
    "    # Adjust y-limit based on the minimum value (with some padding)\n",
    "    min_val = min(values)\n",
    "    plt.ylim(max(0, min_val - 0.1), 1.1)  # Dynamic y-limit with lower bound never below 0\n",
    "\n",
    "    # Add text labels above bars\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.01, f\"{yval:.3f}\",\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    plt.suptitle('Malaria Cell Classification Results (Team 5 Improved)', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout considering suptitle\n",
    "\n",
    "    # Make sure the save directory exists\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # Save the plot\n",
    "    plot_filename = os.path.join(save_path, 'training_validation_test_results_Team5_improved.png')\n",
    "    try:\n",
    "        plt.savefig(plot_filename, dpi=300)  # Higher DPI for better quality\n",
    "        print(f\"\\nResults plot saved to {plot_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving plot: {e}\")\n",
    "\n",
    "    plt.close()  # Close the plot\n",
    "\n",
    "# Call the plotting function\n",
    "plot_results(results, class_names, save_dir)\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
